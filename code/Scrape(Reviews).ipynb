{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before running install the following libraries: pip install bs4, pip install fake_useragent\n",
    "\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from goodreads import client\n",
    "from fake_useragent import UserAgent\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import requests.auth\n",
    "import datetime as dt\n",
    "import time\n",
    "import random\n",
    "import timeit\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Setup\n",
    "ua = UserAgent()\n",
    "\n",
    "gc = client.GoodreadsClient(#insertgoodreadsAPIIDs)\n",
    "\n",
    "#Lists\n",
    "reviewsdf = {}\n",
    "broken = []\n",
    "proxies_list = []\n",
    "\n",
    "\n",
    "#Only tagged to my IP address\n",
    "proxies_listhttps = [#insertproxieshere]\n",
    "\n",
    "proxies_listhttps2 = [#insertproxieshere]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports data\n",
    "df_all = pd.read_csv('books.csv')\n",
    "\n",
    "#Slice data - do not change function \n",
    "def slice_data(start_index,end_index): \n",
    "    df = df_all[start_index:end_index].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def random_proxy():\n",
    "    globals()['randproxy'] = random.randint(1,15)\n",
    "    if randproxy in [5, 10, 15]:\n",
    "        proxies = None\n",
    "    elif randproxy in [1, 2, 3, 4, 6, 7]:\n",
    "        proxies = {'https': random.choice(proxies_listhttps2)}\n",
    "    else:\n",
    "        proxies = {'https': random.choice(proxies_listhttps)}\n",
    "    return proxies\n",
    "\n",
    "def random_headers():\n",
    "    randheader = random.randint(1,10)\n",
    "    if randheader in [5, 10]:\n",
    "        headers = None\n",
    "    else:\n",
    "        headers = {'User-Agent':str(ua.random)}\n",
    "    return headers\n",
    "\n",
    "###Change below - change start and end index\n",
    "start_index = 280000\n",
    "end_index = 285001\n",
    "df = slice_data(start_index,end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews(df):\n",
    "    start = timeit.default_timer()\n",
    "    for x in range(len(df)):\n",
    "        clear_output(wait=True)\n",
    "        headers = random_headers()\n",
    "        proxies = random_proxy()\n",
    "        #Setting values of ISBN13 and retreiving Goodreads book ID\n",
    "        try:\n",
    "            myisbn = int(df.loc[x][1])\n",
    "            book = gc.book(isbn=myisbn)\n",
    "            bookgid = book.gid\n",
    "            df.loc[x, 'gid'] = bookgid\n",
    "        except:\n",
    "            try:\n",
    "                time.sleep(random.randint(3,5))\n",
    "                myisbn = int(df.loc[x][1])\n",
    "                book = gc.book(isbn=myisbn)\n",
    "                bookgid = book.gid\n",
    "                df.loc[x, 'gid'] = bookgid\n",
    "            except:\n",
    "                print(\"Error: Failed to get Goodreads ID\")\n",
    "                broken.append(myisbn)\n",
    "                continue\n",
    "        \n",
    "        #Setting URL for scraping\n",
    "        url = \"https://www.goodreads.com/book/show/\"+str(bookgid)\n",
    "        \n",
    "        #Try to get page\n",
    "        try:\n",
    "            s = requests.Session()\n",
    "            retries = Retry(total=10, backoff_factor=2, status_forcelist=[ 502, 503, 504 ])\n",
    "            s.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "            s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "            r = requests.get(url, headers=headers,timeout=10, proxies=proxies)\n",
    "        except:\n",
    "            try:\n",
    "                headers = {'User-Agent':str(ua.chrome)}\n",
    "                time.sleep(random.randint(3,5))\n",
    "                proxies = random_proxy()\n",
    "                s = requests.Session()\n",
    "                retries = Retry(total=15, backoff_factor=5, status_forcelist=[ 502, 503, 504 ])\n",
    "                s.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "                s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "                r = requests.get(url, headers=headers,timeout=15, proxies=proxies)\n",
    "            except:\n",
    "                try:\n",
    "                    headers = {'User-Agent':str(ua.safari)}\n",
    "                    time.sleep(random.randint(3,5))\n",
    "                    proxies = random_proxy()\n",
    "                    s = requests.Session()\n",
    "                    retries = Retry(total=20, backoff_factor=10, status_forcelist=[ 502, 503, 504 ])\n",
    "                    s.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "                    s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "                    r = requests.get(url, headers=headers,timeout=20, proxies=proxies)\n",
    "                except: \n",
    "                    print(\"Error: Failed to load webpage\")\n",
    "                    broken.append(myisbn)\n",
    "                    continue\n",
    "                \n",
    "        reviews=list()\n",
    "        \n",
    "        #Finding review tags in source code\n",
    "        try:\n",
    "            soup_1 = BeautifulSoup(r.content, \"lxml\")\n",
    "            user = soup_1.find('div', {'id': 'bookReviews'})\n",
    "            user = user.find_all('div',{'class':'friendReviews elementListBrown'})\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                soup_1 = BeautifulSoup(r.content, \"html5lib\")\n",
    "                user = soup_1.find('div', {'id': 'bookReviews'})\n",
    "                user = user.find_all('div', {'class': 'friendReviews elementListBrown'})\n",
    "            except:\n",
    "                print(\"Error: Failed to find reviews\")\n",
    "                pass\n",
    "        \n",
    "        #Scraping name of reviewer and review text \n",
    "        # \"If at first you don't succeed, (try, try, and try again)\" - Lailah Gifty Akita\n",
    "        try:\n",
    "            for row in user:\n",
    "                reviewtext = None    \n",
    "                review = {}\n",
    "                try:\n",
    "                    review['name'] = row.find('a',{'class': 'user'}).text\n",
    "                    try:\n",
    "                        reviewtext = row.find('span',{'style':'display:none'}).text\n",
    "                        review['review'] = reviewtext\n",
    "                    except:\n",
    "                        reviewtext = row.find('span',{'class':'readable'}).text\n",
    "                        review['review'] = reviewtext\n",
    "                    reviews.append(review)\n",
    "                    \n",
    "                except:\n",
    "                    continue\n",
    "        except:\n",
    "            broken.append(myisbn)\n",
    "            print(\"Error: Failed to find reviews\")\n",
    "            continue\n",
    "        \n",
    "        rdf = pd.DataFrame(reviews, columns=['isbn13', 'gid', 'name', 'review'])\n",
    "        rdf['gid'] = bookgid\n",
    "        rdf['isbn13'] = myisbn\n",
    "        reviewsdf[str(myisbn)]=rdf\n",
    "            \n",
    "        stop =  timeit.default_timer()  \n",
    "              \n",
    "        #Random rest time after every 50 books\n",
    "        if x % 50 == 0: \n",
    "            time.sleep(random.randint(5,10))\n",
    "        \n",
    "        #Times\n",
    "        if x < 10: \n",
    "                expected_time = \"Calculating...\" \n",
    "        else: \n",
    "            stop1 = timeit.default_timer()\n",
    "            expected_time = np.round( ( (stop1-start)/(x/len(df)) )/60, 2)\n",
    "\n",
    "        print('Current progress:', np.round((x+1)/len(df)*100, 2),\"%\")\n",
    "        print('Current progress: {} out of {} books.'.format(x+1,len(df)))\n",
    "        print('Total run time:', np.round((stop-start)/60, 2), \"minutes\")\n",
    "        print('Expected run time:', expected_time, \"minutes\")\n",
    "        print('Number of broken entries: ', len(broken), \"(\", np.round(100*len(broken)/(x+1), 2),\"%)\")\n",
    "        print(\"Proxy Used: \", proxies, \"(\", randproxy, \")\")\n",
    "        print(\"Headers Used: \", headers)\n",
    "        if x % 5 == 0:\n",
    "            try:\n",
    "                ipaddress = requests.get('https://bot.whatismyipaddress.com/', headers=headers,timeout=15, proxies=proxies)\n",
    "                print (\"IP Address Used:\",ipaddress.text)\n",
    "            except:\n",
    "                print(\"Error: Failed to find IP address\")\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current progress: 100.0 %\n",
      "Current progress: 5001 out of 5001 books.\n",
      "Total run time: 631.64 minutes\n",
      "Expected run time: 631.9 minutes\n",
      "Number of broken entries:  2 ( 0.04 %)\n",
      "Proxy Used:  {'https': 'http://jrkuah:N0G00Dingoodbye@104.144.54.88:2867'} ( 4 )\n",
      "Headers Used:  {'User-Agent': 'Mozilla/5.0 (X11; CrOS i686 4319.74.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.57 Safari/537.36'}\n",
      "IP Address Used: 104.144.54.88\n"
     ]
    }
   ],
   "source": [
    "get_reviews(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were  2 entries.\n",
      "Broken entries:\n",
      "[9781606905067, 9781721365272]\n"
     ]
    }
   ],
   "source": [
    "print('There were ', len(broken), 'entries.')\n",
    "print('Broken entries:')\n",
    "print(broken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of reviews per book : 16.433286657331465\n"
     ]
    }
   ],
   "source": [
    "isbn13 = []\n",
    "countreviews = []\n",
    "\n",
    "for x in range(len(df)):\n",
    "    myisbn = int(df.loc[x][1])\n",
    "    isbn13.append(myisbn)\n",
    "    \n",
    "for isbn in isbn13:\n",
    "    try:\n",
    "        count = len(reviewsdf[str(isbn)]['review'])\n",
    "        countreviews.append(count)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print('Average number of reviews per book :', sum(countreviews)/len(countreviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.DataFrame(columns=['isbn13', 'gid', 'name', 'review'])\n",
    "\n",
    "for x in range(len(df)):\n",
    "    try:\n",
    "        myisbn = int(df.loc[x][1])\n",
    "        combined = combined.append(reviewsdf[str(myisbn)])\n",
    "        combined = combined.reset_index(drop=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "combined.to_csv('output_' + str(start_index) + '_' + str(end_index) + '.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
